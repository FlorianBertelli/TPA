{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "#import nltk\n",
    "from collections import Counter \n",
    "import matplotlib.pyplot as plt\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On génère les série temporelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_first_type_time_series(D):\n",
    "    return np.array([[np.sin(np.divide(2*np.pi*i*t, 64)) for t in range(128)] for i in range(1,D+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_second_type_time_series(D):\n",
    "    fst_type = generate_first_type_time_series(D)\n",
    "    result = []\n",
    "    frac = 1/(D-1)\n",
    "    for i in range(1,D+1):\n",
    "        loc = []\n",
    "        for t in range(1,129):\n",
    "            fst = fst_type[i-1,t-1]\n",
    "            snd=frac*np.sum(np.delete(fst_type[:,t-1], i-1, axis=0))\n",
    "            loc.append(fst+snd)\n",
    "        result.append(loc)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(D, MTS):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    if MTS:\n",
    "        series = generate_second_type_time_series(D)\n",
    "    else : \n",
    "        series = generate_first_type_time_series(D)\n",
    "     \n",
    "    for serie in series :\n",
    "        for i in range(0,64):\n",
    "            data.append(serie[i:i+64])\n",
    "            labels.append(serie[i+64])\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels) \n",
    "    train_data = TensorDataset(th.from_numpy(data).type(th.LongTensor), th.from_numpy(labels))\n",
    "    train_loader = DataLoader(train_data, shuffle=False, batch_size=1)\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_forecaster(nn.Module):\n",
    "    def __init__(self, nb_cells, hidden_size, input_size, rnn_dropout, window_size, bidirectional=False):\n",
    "        super(GRU_forecaster, self).__init__()\n",
    "        \n",
    "        self.nb_cells = nb_cells\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.is_bidirectional = bidirectional\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, nb_cells, batch_first = True, dropout=rnn_dropout,bidirectional=bidirectional)\n",
    "\n",
    "        if bidirectional:    \n",
    "            self.fc = nn.Linear(2*hidden_size, 1)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        ###Par rapport au papier, k=nbr de filtres et m=hidden size\n",
    "        self.convList = nn.ModuleList([nn.Conv1d(1,1,window_size-1) for i in range(32)])#32 fixed in the paper\n",
    "\n",
    "        self.Wa = nn.Linear(hidden_size, 32)\n",
    "        self.Wh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Wv = nn.Linear(32, hidden_size)\n",
    "        self.Whp = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc.weight.data)  # Xavier/Glorot init for tanh\n",
    "        nn.init.zeros_(self.fc.bias.data)  # Xavier/Glorot init for tanh\n",
    "        \n",
    "            \n",
    "    \n",
    "    def forward(self, inputs, h0):\n",
    "        inputs = inputs.view(1,-1,1).float()\n",
    "        batch_size = inputs.size(0)\n",
    "        rnn_out, h = self.gru(inputs, h0)\n",
    "        hidden = rnn_out.transpose(1,2)[:,:,:-1]\n",
    "        last_h = rnn_out[0,-1]\n",
    "        hc = th.Tensor() #size m*k\n",
    "        #l_out = self.getNbFeatures()\n",
    "        \n",
    "        for conv in self.convList:\n",
    "            t = th.Tensor()\n",
    "            for h in hidden[0]:\n",
    "                h=h.view(1,1,-1)\n",
    "                x = conv(h)\n",
    "                t=th.cat([t,x],0)\n",
    "            t = t.view(-1,1)\n",
    "            hc=th.cat([hc,t],1)\n",
    "        \n",
    "        attention_weights = self.Wa(last_h).view(-1,1)\n",
    "        attention_weights = (hc@attention_weights)\n",
    "        attention_weights = self.sigmoid(attention_weights)\n",
    "        \n",
    "        \n",
    "        vt = (attention_weights*hc).sum(dim=0) #ai * Hci\n",
    "        \n",
    "        \n",
    "        hp = self.Wv(vt)+self.Wh(last_h)\n",
    "        out = self.Whp(hp)\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        #hidden = th.Tensor(self.nb_cells, batch_size, self.hidden_size)\n",
    "        if self.is_bidirectional:\n",
    "            hidden = weight.new(2*self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "        else:\n",
    "            hidden = weight.new(self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "        return hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cells=1\n",
    "learning_rate = 1e-2\n",
    "hidden_size=150\n",
    "input_size=1\n",
    "gru=GRU_forecaster(nb_cells,hidden_size, input_size, 0.0,64)\n",
    "loss_fn = nn.L1Loss()\n",
    "D=6\n",
    "loader = get_loader(D, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model,lr, max_epochs=200):\n",
    "    optim = th.optim.Adam(params=model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    model.train()\n",
    "    for i in range(max_epochs):\n",
    "        n = 0 \n",
    "        h = model.init_hidden(1)\n",
    "        train_mean_loss=0\n",
    "        for x, labels in loader:\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            h = h.data\n",
    "            \n",
    "            n+=1\n",
    "            preds, _ = model(x, h)  \n",
    "            \n",
    "            preds=preds.view(1,-1)\n",
    "            labels=labels.view(1,-1).float()\n",
    "            loss = loss_fn(preds, labels)\n",
    "            h = h.detach()\n",
    "            train_mean_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optim.step()\n",
    "        \n",
    "            if(n%20==0):\n",
    "                print(\"step : {}/{} \".format(n, 64*D))\n",
    "                print(\"step loss : \", log(train_mean_loss/n,10))  \n",
    "        \n",
    "#         rnn_train_losses.append(train_mean_loss/len(train_data))\n",
    "        \n",
    "        print(\"EPOCH {}\".format(i+1))\n",
    "        print(\"Train Mean loss : \",log(train_mean_loss/n,10))\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 20/384 \n",
      "step loss :  0.40367230204071036\n",
      "step : 40/384 \n",
      "step loss :  0.15602928429900448\n",
      "step : 60/384 \n",
      "step loss :  0.011942997414283327\n",
      "step : 80/384 \n",
      "step loss :  -0.08698546005476424\n",
      "step : 100/384 \n",
      "step loss :  -0.12861896671932857\n",
      "step : 120/384 \n",
      "step loss :  -0.15857198748649795\n",
      "step : 140/384 \n",
      "step loss :  -0.19866356670254323\n",
      "step : 160/384 \n",
      "step loss :  -0.20719121142180325\n",
      "step : 180/384 \n",
      "step loss :  -0.22311990324481182\n",
      "step : 200/384 \n",
      "step loss :  -0.23483106826459177\n",
      "step : 220/384 \n",
      "step loss :  -0.10811409213536774\n",
      "step : 240/384 \n",
      "step loss :  0.3918052067903433\n",
      "step : 260/384 \n",
      "step loss :  0.39986060619478525\n"
     ]
    }
   ],
   "source": [
    "train(gru, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
